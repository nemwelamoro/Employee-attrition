import pandas as pd
# Load the dataset
df = pd.read_csv("F:/IBM.csv")
df.head()

# Analyze the 'Attrition' variable
attrition_counts = df['Attrition'].value_counts()

print(attrition_counts)

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit label encoder and return encoded labels
df['Attrition'] = le.fit_transform(df['Attrition'])

# Display the first few rows of the dataframe
print(df[['Attrition']].head())

# Calculate the percentage of missing values for each column
missing_percentage = df.isnull().mean() * 100

# Print the percentage of missing values
print("Percentage of missing values per column:")
print(missing_percentage)

# Identify columns where more than 50% of the values are missing
columns_to_drop = missing_percentage[missing_percentage > 50].index

# Drop these columns from the DataFrame
df.drop(columns=columns_to_drop, inplace=True)

# Verify columns have been dropped
print("\nColumns remaining after dropping those with >50% missing values:")
print(df.columns)


#numeric variables
df_numeric= df[["Age","DistanceFromHome","MonthlyIncome","NumCompaniesWorked","YearsAtCompany"]]
    
# Calculate the correlation matrix
corr_matrix = df_numeric.corr()

# Display the correlation matrix
print("Correlation matrix of numeric variables:")
print(corr_matrix)

# Identify pairs with high correlation for potential multicollinearity issues
high_corr_vars = set()
threshold = 0.8  # or 0.9, depending on your preference

for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            colname = corr_matrix.columns[i]
            high_corr_vars.add(colname)

# If there are any highly correlated pairs, decide on which one to remove
# This requires domain knowledge and considering the importance of variables in your analysis
print("\nVariables to consider removing due to high correlation:")
print(high_corr_vars)

# List of categorical variables to dummify
categorical_vars = ['Department', 'EducationField', 'MaritalStatus'] 

# Create dummy variables for the categorical variables and drop the first level
df_dummies = pd.get_dummies(df, columns=categorical_vars, drop_first=True)

# Display the first few rows to verify
print(df_dummies.head())

from sklearn.model_selection import train_test_split

# Define your features (X) and the target variable (y)
X = df_dummies.drop('Attrition', axis=1)  
y = df_dummies['Attrition']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=3)  # Example seed

# Print the size of each set to verify
print(f"Training set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")

# use 'df_dummies' it's already been processed for categorical variables
grouped_means = df_dummies.groupby('Attrition').mean()
print(grouped_means)

import statsmodels.api as sm

# Add a constant to the independent variable set to account for the intercept
X_train_const = sm.add_constant(X_train)

# Building the logistic regression model
logit_model = sm.Logit(y_train, X_train_const)

# Fitting the model
result = logit_model.fit()

# Displaying the summary of the logistic regression model
print(result.summary())


import statsmodels.api as sm

# Selecting variables that were deemed significant or didn't have high p-values for all levels
X = X_train[['Age', 'DistanceFromHome', 'EnvironmentSatisfaction', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'YearsAtCompany', 'MaritalStatus_Single']]
y = y_train

# Adding a constant to the model (intercept)
X = sm.add_constant(X)

# Building the logistic regression model
log_reg = sm.Logit(y, X).fit()

# Displaying the summary of the model
print(log_reg.summary())


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix



# Selecting the features based on the final model from statsmodels
features = ['Age', 'DistanceFromHome', 'EnvironmentSatisfaction', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'YearsAtCompany', 'MaritalStatus_Single']
X = df_dummies[features]
y = df_dummies['Attrition']  

# Splitting the data into training and test sets based on your chosen random_state
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=3)  

# Creating the logistic regression model
log_reg_sklearn = LogisticRegression(max_iter=1000)  

# Fitting the model to the training data
log_reg_sklearn.fit(X_train, y_train)


# Making predictions on the test set
y_pred = log_reg_sklearn.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, balanced_accuracy_score

# Generating the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
TN, FP, FN, TP = conf_matrix.ravel()

# Calculating the metrics
sensitivity_rate = TP / (TP + FN)  # Also known as recall

print(f"Sensitivity Rate (Recall): {sensitivity_rate}")

specificity_rate = TN / (TN + FP)

print(f"Specificity Rate: {specificity_rate}")


precision = precision_score(y_test, y_pred)

print(f"Precision: {precision}")


balanced_accuracy = balanced_accuracy_score(y_test, y_pred)

print(f"Balanced Accuracy: {balanced_accuracy}")

# Calculate accuracy on the training set
y_train_pred = log_reg_sklearn.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)

#  already calculated the test accuracy previously
test_accuracy = accuracy 
print(f"Training Set Accuracy: {train_accuracy}")
print(f"Test Set Accuracy: {test_accuracy}")


import pandas as pd

# Example values for the hypothetical employee
data = {
    'Age': [28],  # Age of the employee
    'DistanceFromHome': [10],  # Distance from home in km
    'EnvironmentSatisfaction': [3],  # Satisfaction with the environment (1-4 scale)
    'JobSatisfaction': [2],  # Satisfaction with the job (1-4 scale)
    'MonthlyIncome': [5000],  # Monthly income in USD
    'NumCompaniesWorked': [2],  # Number of companies worked at
    'YearsAtCompany': [3],  # Number of years at the current company
    'MaritalStatus_Single': [1]  # Marital status (0 = Not single, 1 = Single)
}

# Create a DataFrame
employee_df = pd.DataFrame(data)

# our model is named log_reg_sklearn and is already trained
employee_pred = log_reg_sklearn.predict(employee_df)

# Output prediction
print("Prediction (0 = Stay, 1 = Leave):", employee_pred[0])

#probability of leaving
employee_pred_proba = log_reg_sklearn.predict_proba(employee_df)
# Output probability of leaving
print("Probability of Leaving:", employee_pred_proba[0][1])

import pandas as pd

# Create a new dataframe with out-of-range values
data = {
    'Age': [100],
    'DistanceFromHome': [300],
    'EnvironmentSatisfaction': [3],  # Assume within normal range
    'JobSatisfaction': [3],  # Assume within normal range
    'MonthlyIncome': [50000],
    'NumCompaniesWorked': [15],
    'YearsAtCompany': [50],
    'MaritalStatus_Single': [0]  # Assume within normal range
}

new_employee_df = pd.DataFrame(data)

# Assuming 'logreg' is our trained logistic regression model
prediction = log_reg_sklearn.predict(new_employee_df)

print("Prediction for the new employee (0 = Stay, 1 = Leave):", prediction)

import pandas as pd
# Load the dataset
df = pd.read_csv("F:/IBM.csv")
df.head()

# List of categorical variables to dummify
categorical_vars = ['Department', 'EducationField', 'MaritalStatus'] 

# Create dummy variables for the categorical variables and drop the first level
df_dummies = pd.get_dummies(df, columns=categorical_vars, drop_first=False)

# Convert 'Attrition' into binary format using LabelEncoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_dummies['Attrition'] = le.fit_transform(df_dummies['Attrition'])

# Display the first few rows to verify
print(df_dummies.head())

from sklearn.model_selection import train_test_split

# the seed value used earlier was 3 
X = df_dummies.drop('Attrition', axis=1)
y = df_dummies['Attrition']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=3)


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300], # Number of trees
    'max_depth': [None, 10, 20, 30], # Maximum depth of the trees
    'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split an internal node
}

# Initialize the random forest classifier
rf = RandomForestClassifier(random_state=42)

# Initialize the GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit the model
grid_search.fit(X_train, y_train)

# Best hyperparameters
best_params = grid_search.best_params_
print("Best parameters:", best_params)

# Best model
best_rf = grid_search.best_estimator_


import numpy as np
# Get feature importances from the random forest model
importances = best_rf.feature_importances_

# Sort the feature importances in descending order
indices = np.argsort(importances)[::-1]

print("Feature ranking:")
for f in range(X_train.shape[1]):
    print(f"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})")


from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, balanced_accuracy_score

# Make predictions with the test set
y_pred = best_rf.predict(X_test)

# Build the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)

# Print metrics
print(f"a. Accuracy Rate: {accuracy}")

sensitivity = recall_score(y_test, y_pred)  # also known as recall

print(f"b. Sensitivity Rate (Recall): {sensitivity}")

specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

print(f"c. Specificity Rate: {specificity}")

precision = precision_score(y_test, y_pred)

print(f"d. Precision: {precision}")

balanced_accuracy = balanced_accuracy_score(y_test, y_pred)

print(f"e. Balanced Accuracy: {balanced_accuracy}")


# Calculate accuracy on the training set
y_train_pred = best_rf.predict(X_train)
accuracy_train = accuracy_score(y_train, y_train_pred)
print(f"Accuracy on the Training Set: {accuracy_train}")

# The accuracy on the test set was previously calculated, but let's state it again for comparison
print(f"Accuracy on the Test Set: {accuracy}")


new_employee_data = {
    'Age': [28],
    'DistanceFromHome': [10],
    'EnvironmentSatisfaction': [3],
    'JobSatisfaction': [2],
    'MonthlyIncome': [5000],
    'NumCompaniesWorked': [2],
    'YearsAtCompany': [3],
    'MaritalStatus_Single': [1],
    'MaritalStatus_Married': [0],
    'MaritalStatus_Divorced': [0],
    'Department_Sales': [0],  # Assuming not specified, hence not in Sales
    'Department_Research & Development': [0],  # Assuming not specified
    'Department_Human Resources': [0],  # Assuming not specified
    'EducationField_Marketing': [0],  # Assuming not specified
    'EducationField_Medical': [0],  # Assuming not specified
    'EducationField_Life Sciences': [0],  # Assuming not specified
    'EducationField_Technical Degree': [0],  # Assuming not specified
    'EducationField_Other': [0],  # Assuming not specified
    'EducationField_Human Resources': [0],  # Assuming not specified
    'WorkLifeBalance': [3],  # Assuming a neutral work-life balance
    'Education': [3]  # Assuming a Bachelor's degree level
}

# Convert to DataFrame
df_new_employee = pd.DataFrame(new_employee_data)

# Assuming 'best_rf' is your best-performing random forest model after GridSearchCV
predicted_outcome_rf = best_rf.predict(df_new_employee)

# Check if the model thinks the employee will leave
if predicted_outcome_rf[0] == 1:
    print("The model predicts that the employee will leave.")
else:
    print("The model predicts that the employee will stay.")