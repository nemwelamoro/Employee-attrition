import pandas as pd
# Load the dataset
df = pd.read_csv("F:/IBM.csv")
df.head()

# Analyze the 'Attrition' variable
attrition_counts = df['Attrition'].value_counts()

print(attrition_counts)

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit label encoder and return encoded labels
df['Attrition'] = le.fit_transform(df['Attrition'])

# Display the first few rows of the dataframe
print(df[['Attrition']].head())

# Calculate the percentage of missing values for each column
missing_percentage = df.isnull().mean() * 100

# Print the percentage of missing values
print("Percentage of missing values per column:")
print(missing_percentage)

# Identify columns where more than 50% of the values are missing
columns_to_drop = missing_percentage[missing_percentage > 50].index

# Drop these columns from the DataFrame
df.drop(columns=columns_to_drop, inplace=True)

# Verify columns have been dropped
print("\nColumns remaining after dropping those with >50% missing values:")
print(df.columns)


#numeric variables
df_numeric= df[["Age","DistanceFromHome","MonthlyIncome","NumCompaniesWorked","YearsAtCompany"]]
    
# Calculate the correlation matrix
corr_matrix = df_numeric.corr()

# Display the correlation matrix
print("Correlation matrix of numeric variables:")
print(corr_matrix)

# Identify pairs with high correlation for potential multicollinearity issues
high_corr_vars = set()
threshold = 0.8  # or 0.9, depending on your preference

for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            colname = corr_matrix.columns[i]
            high_corr_vars.add(colname)

# If there are any highly correlated pairs, decide on which one to remove
# This requires domain knowledge and considering the importance of variables in your analysis
print("\nVariables to consider removing due to high correlation:")
print(high_corr_vars)

# List of categorical variables to dummify
categorical_vars = ['Department', 'EducationField', 'MaritalStatus'] 

# Create dummy variables for the categorical variables and drop the first level
df_dummies = pd.get_dummies(df, columns=categorical_vars, drop_first=True)

# Display the first few rows to verify
print(df_dummies.head())

from sklearn.model_selection import train_test_split

# Define your features (X) and the target variable (y)
X = df_dummies.drop('Attrition', axis=1)  
y = df_dummies['Attrition']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=3)  # Example seed

# Print the size of each set to verify
print(f"Training set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")

# use 'df_dummies' it's already been processed for categorical variables
grouped_means = df_dummies.groupby('Attrition').mean()
print(grouped_means)

import statsmodels.api as sm

# Add a constant to the independent variable set to account for the intercept
X_train_const = sm.add_constant(X_train)

# Building the logistic regression model
logit_model = sm.Logit(y_train, X_train_const)

# Fitting the model
result = logit_model.fit()

# Displaying the summary of the logistic regression model
print(result.summary())


import statsmodels.api as sm

# Selecting variables that were deemed significant or didn't have high p-values for all levels
X = X_train[['Age', 'DistanceFromHome', 'EnvironmentSatisfaction', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'YearsAtCompany', 'MaritalStatus_Single']]
y = y_train

# Adding a constant to the model (intercept)
X = sm.add_constant(X)

# Building the logistic regression model
log_reg = sm.Logit(y, X).fit()

# Displaying the summary of the model
print(log_reg.summary())


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix



# Selecting the features based on the final model from statsmodels
features = ['Age', 'DistanceFromHome', 'EnvironmentSatisfaction', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'YearsAtCompany', 'MaritalStatus_Single']
X = df_dummies[features]
y = df_dummies['Attrition']  

# Splitting the data into training and test sets based on your chosen random_state
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=3)  

# Creating the logistic regression model
log_reg_sklearn = LogisticRegression(max_iter=1000)  

# Fitting the model to the training data
log_reg_sklearn.fit(X_train, y_train)


# Making predictions on the test set
y_pred = log_reg_sklearn.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, balanced_accuracy_score

# Generating the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
TN, FP, FN, TP = conf_matrix.ravel()

# Calculating the metrics
sensitivity_rate = TP / (TP + FN)  # Also known as recall

print(f"Sensitivity Rate (Recall): {sensitivity_rate}")

specificity_rate = TN / (TN + FP)

print(f"Specificity Rate: {specificity_rate}")


precision = precision_score(y_test, y_pred)

print(f"Precision: {precision}")


balanced_accuracy = balanced_accuracy_score(y_test, y_pred)

print(f"Balanced Accuracy: {balanced_accuracy}")

# Calculate accuracy on the training set
y_train_pred = log_reg_sklearn.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)

#  already calculated the test accuracy previously
test_accuracy = accuracy 
print(f"Training Set Accuracy: {train_accuracy}")
print(f"Test Set Accuracy: {test_accuracy}")