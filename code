import pandas as pd
# Load the dataset
df = pd.read_csv("F:/IBM.csv")
df.head()

# Analyze the 'Attrition' variable
attrition_counts = df['Attrition'].value_counts()

print(attrition_counts)

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit label encoder and return encoded labels
df['Attrition'] = le.fit_transform(df['Attrition'])

# Display the first few rows of the dataframe
print(df[['Attrition']].head())

# Calculate the percentage of missing values for each column
missing_percentage = df.isnull().mean() * 100

# Print the percentage of missing values
print("Percentage of missing values per column:")
print(missing_percentage)

# Identify columns where more than 50% of the values are missing
columns_to_drop = missing_percentage[missing_percentage > 50].index

# Drop these columns from the DataFrame
df.drop(columns=columns_to_drop, inplace=True)

# Verify columns have been dropped
print("\nColumns remaining after dropping those with >50% missing values:")
print(df.columns)


#numeric variables
df_numeric= df[["Age","DistanceFromHome","MonthlyIncome","NumCompaniesWorked","YearsAtCompany"]]
    
# Calculate the correlation matrix
corr_matrix = df_numeric.corr()

# Display the correlation matrix
print("Correlation matrix of numeric variables:")
print(corr_matrix)

# Identify pairs with high correlation for potential multicollinearity issues
high_corr_vars = set()
threshold = 0.8  # or 0.9, depending on your preference

for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            colname = corr_matrix.columns[i]
            high_corr_vars.add(colname)

# If there are any highly correlated pairs, decide on which one to remove
# This requires domain knowledge and considering the importance of variables in your analysis
print("\nVariables to consider removing due to high correlation:")
print(high_corr_vars)